{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "waBjAgsuk4EJ",
    "outputId": "e5b4ae55-7d67-4556-e770-1344081ed865"
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fr8cBNXHjkBA",
    "outputId": "829b5bbc-d643-4ef0-d8e8-ffff844325e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def slice_audio(input_folder, output_folder, slice_duration_ms=10000):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".mp3\") or filename.endswith(\".wav\"):\n",
    "            path = os.path.join(input_folder, filename)\n",
    "            audio = AudioSegment.from_file(path)\n",
    "\n",
    "            # Slice audio into segments\n",
    "            for i, chunk in enumerate(audio[::slice_duration_ms]):\n",
    "                if len(chunk) < slice_duration_ms:\n",
    "                    continue # Skip clips that are too short\n",
    "\n",
    "                output_filename = f\"{os.path.splitext(filename)[0]}_clip_{i}.wav\"\n",
    "                chunk.export(os.path.join(output_folder, output_filename), format=\"wav\")\n",
    "                print(f\"Exported: {output_filename}\")\n",
    "\n",
    "# Run the slicer\n",
    "slice_audio(\"/content/input_folder\", \"qaraami_dataset_clips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANGTcBrQorqy",
    "outputId": "1c45152e-b121-421f-b56b-1c9f27c7ea00"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# This is a template for your first 3 clips.\n",
    "# You will need to manually adjust the 'notes' to match what is actually played.\n",
    "dataset_labels = [\n",
    "    {\n",
    "        \"audio_file\": \"audio/clip_001.wav\",\n",
    "        \"instrument\": \"oud\",\n",
    "        \"scale\": \"somali_pentatonic\",\n",
    "        \"notes\": [\n",
    "            {\"pitch\": \"C4\", \"start_time\": 0.5, \"end_time\": 1.2},\n",
    "            {\"pitch\": \"D4\", \"start_time\": 1.3, \"end_time\": 2.0},\n",
    "            {\"pitch\": \"F4\", \"start_time\": 2.1, \"end_time\": 3.5}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"audio_file\": \"audio/clip_002.wav\",\n",
    "        \"instrument\": \"oud\",\n",
    "        \"scale\": \"somali_pentatonic\",\n",
    "        \"notes\": [\n",
    "            {\"pitch\": \"G4\", \"start_time\": 0.2, \"end_time\": 1.0},\n",
    "            {\"pitch\": \"A4\", \"start_time\": 1.1, \"end_time\": 2.5}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ensure the 'dataset' directory exists\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "\n",
    "# Save as a JSONL file for Hugging Face or PyTorch\n",
    "with open('dataset/metadata.jsonl', 'w') as f:\n",
    "    for entry in dataset_labels:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(\"Success: metadata.jsonl created in /dataset folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooXCvy4_o1XW",
    "outputId": "b659c82c-dc5d-403a-96d2-7de4ae6aec1e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HY6Gv4IhqTRx",
    "outputId": "0d1c9a0f-3025-460e-cd10-a040ca7c20f2"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# lets make a loop\n",
    "audio_folder = 'dataset/audio'\n",
    "qaraami_dataset_clips = '/content/qaraami_dataset_clips'\n",
    "\n",
    "# Loop through every file in that folder\n",
    "for filename in os.listdir(qaraami_dataset_clips):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        file_path = os.path.join(qaraami_dataset_clips, filename)\n",
    "\n",
    "        # Load the specific clip\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        # Extract the pitches (Fundamental Frequency)\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "\n",
    "        # Filter for the strongest notes\n",
    "        pitch_values = pitches[magnitudes > np.max(magnitudes) * 0.1]\n",
    "\n",
    "        if len(pitch_values) > 0:\n",
    "            avg_hz = np.mean(pitch_values)\n",
    "            print(f\"File: {filename} | Frequency: {avg_hz:.2f} Hz\")\n",
    "        else:\n",
    "            print(f\"File: {filename} | Could not detect clear pitch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wk3secXMo5G-",
    "outputId": "edc5b2f7-41d4-4fa7-debf-fa93765a1e3d"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "def hz_to_note_name(hz):\n",
    "    if hz <= 0: return None\n",
    "    # librosa.hz_to_note converts frequency to musical notation\n",
    "    return librosa.hz_to_note(hz)\n",
    "\n",
    "audio_folder = 'qaraami_dataset_clips'\n",
    "# Create the audio folder if it doesn't exist\n",
    "os.makedirs(audio_folder, exist_ok=True)\n",
    "training_data = []\n",
    "\n",
    "print(\"--- Starting Qaraami Pitch Mapping ---\")\n",
    "\n",
    "for filename in sorted(os.listdir(audio_folder)):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        file_path = os.path.join(audio_folder, filename)\n",
    "\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(file_path)\n",
    "\n",
    "        # Track pitch over time\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "\n",
    "        # Extract the strongest frequency\n",
    "        pitch_values = pitches[magnitudes > np.max(magnitudes) * 0.1]\n",
    "\n",
    "        if len(pitch_values) > 0:\n",
    "            avg_hz = np.mean(pitch_values)\n",
    "            note = hz_to_note_name(avg_hz)\n",
    "\n",
    "            print(f\"File: {filename} | {avg_hz:.2f} Hz -> Note: {note}\")\n",
    "\n",
    "            # Prepare the JSON structure for training\n",
    "            training_data.append({\n",
    "                \"audio_file\": f\"audio/{filename}\",\n",
    "                \"pitch_hz\": float(round(avg_hz, 2)), # Convert numpy.float32 to standard float\n",
    "                \"note\": note,\n",
    "                \"scale\": \"somali_pentatonic\" # Explicitly marking the target scale\n",
    "            })\n",
    "\n",
    "# Save the final metadata for the machine to read\n",
    "with open('dataset/metadata.jsonl', 'w') as f:\n",
    "    for entry in training_data:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"\\nSuccess! metadata.jsonl created with {len(training_data)} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a484dc78",
    "outputId": "15ecbdbe-7e4c-4191-bce8-a359ab78cb75"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dataset/metadata.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        print(json.dumps(json.loads(line), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnJPgya6z3xo",
    "outputId": "2ac97c8e-7120-46dd-9e28-b3c8d92e4b80"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "e04909d764f54d508a36b87a23fe5a2c",
      "008da34a80714de2968b674e5475aee5",
      "9d6719303145415394537730e7ea3b0a",
      "14dfcfdbba1f4669b44edfe736f4169a",
      "b30a854b0e9747d486504502c821d7da",
      "3a322787d0424b4e843b8c285efb9ee3",
      "31b84a46193a48f7b6f3e8b6a7d750d7",
      "a115dc7f9ac14fd7a049840aa03b88a9",
      "897012e88a3e4fa592869c83644facb6",
      "14c42c65b81243928ae8c39a48630b98",
      "3b95b78484a342eb95683d196b7e1398",
      "cbaa5875617b497d84fdb24d43d3773f",
      "06f77d435c244fc0b82df6671daa2d05",
      "df62b44aa4154f01becdb7184f99bbea",
      "081023066231458c8e277e8937374822",
      "6a0e71ef27cb4df28ae83191553e3db1",
      "9f1ce08952a941cca5935012ff5449f9",
      "f6a579ed55fb4800ad77f4bcd17fb5ab",
      "875bab8a9c8544248c490d8b7117e641",
      "d1fc978a9d9e412b8246fcad404e1031"
     ]
    },
    "id": "1mL7eRHWz8nQ",
    "outputId": "1b706484-1340-4aa4-c5ad-bb239e4c902d"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eor6X2b-4pr6"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "Qaraami= userdata.get('Qaraami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3u32iJdr1chi",
    "outputId": "310dd0ab-a741-4044-fa26-cda60e15cebd"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# 1. Configuration\n",
    "# Replace 'khalidibrahim' with your actual Hugging Face username\n",
    "# The repo_id will be the name of your new dataset on HF\n",
    "api = HfApi()\n",
    "repo_id = \"khalidibra/Qaraami-Colab-Write\"\n",
    "\n",
    "# 2. Create the repository on Hugging Face (it's okay if it already exists)\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# 3. Upload the entire folder\n",
    "# This includes the 'audio/' subfolder and the 'metadata.jsonl' file\n",
    "print(f\"Uploading dataset to https://huggingface.co/datasets/{repo_id}...\")\n",
    "api.upload_folder(\n",
    "    folder_path=\"dataset\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(\"Upload Complete! Your dataset is now ready for the training phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au50hYco5yYN"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers[torch] accelerate librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "e4bb396156b84181a3cae158c0907608",
      "cda78230aba04ed9b07df2b5b51384d8",
      "a362349ed764496eb726aef642ca9b66",
      "982adbe261f44421893648b93f6b6cd1",
      "dc0e54a3a12242ca9a433554e43d3bb9",
      "a821e515df3743d19222b4c9f16ae29e",
      "cd882e4f1c1d4e22ab51ed13d8e1dd57",
      "7ab4856384ca44a3af463c9e2de8bba6",
      "11c9ab3725424f98827af8ac8818e89e",
      "83019cbc33524fabb243af0e33d68503",
      "a8e72987fc7f4e819e2ea88003391a3e",
      "2720c3c1c0dc4636ab27540dd8a2a818",
      "dff3b35cb3784db78ee51b35f19cf6ef",
      "a955eb3a7563473a921974d46d3fbc73",
      "0b2e3e22396f4e44ab003d6a4444a065",
      "1f537c6d69f3466983c2002a2b6b3417",
      "2a9905b27fe14d988c2ca29c2baeca22",
      "83f5d37e65ec4babb6748124fb8b7a8e",
      "736763a336f64b3b82d038a5f27fdc61",
      "c08243b5ec1e4a50867a86463623f32e",
      "5c1745ed7382430484703674beb0da14",
      "5e7db1a7454f4828826fab4109979710"
     ]
    },
    "id": "Rm1gSGfu58wE",
    "outputId": "0fbc739c-d46a-4380-e614-ebf41c44fce4"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This pulls your 50 clips directly from the cloud into Colab memory\n",
    "dataset = load_dataset(\"khalidibra/Qaraami-Colab-Write\", split=\"train\")\n",
    "\n",
    "print(f\"Total Clips: {len(dataset)}\")\n",
    "print(f\"Sample Note: {dataset[0]['note']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "pWAYLp4o6m6P",
    "outputId": "3f27e717-4c8e-4e89-f43c-99490af9c73a"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_qaraami_spectrogram(audio_path):\n",
    "    # 1. Load the audio file\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # 2. Compute the Mel-scaled spectrogram\n",
    "    # We use 128 Mel bands for detailed frequency resolution\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "\n",
    "    # 3. Convert to log scale (decibels)\n",
    "    # This makes the patterns more visible for machine learning\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # 4. Display the result\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    img = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
    "    plt.colorbar(img, format='%+2.0f dB')\n",
    "    plt.title(f'Qaraami Mel-Spectrogram: {audio_path.split(\"/\")[-1]}')\n",
    "    plt.show()\n",
    "\n",
    "# Test it on your first extracted clip\n",
    "visualize_qaraami_spectrogram('/content/qaraami_dataset_clips/extracted_audio_clip_0.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGMJ6Skv7QqX",
    "outputId": "faa2b3ef-d6d4-4581-b9dc-046cd0945e77"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QaraamiTranscriptor(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(QaraamiTranscriptor, self).__init__()\n",
    "        # Layer 1: Detects basic edges and lines in the spectrogram\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # Layer 2: Detects more complex Oud-specific textures\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Global Pooling to condense features\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Final output layer: Maps features to your Somali notes\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128) # Size depends on spectrogram input\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1) # Flatten for the final decision\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate for your 50-note dataset\n",
    "model = QaraamiTranscriptor(num_classes=50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWE3Dw3H8Tuk",
    "outputId": "01c354f7-1058-4cfb-a366-9bd47af35d47"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # Standard size for CNN input\n",
    "    transforms.ToTensor(),         # Scales values to [0.0, 1.0]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Centers data to [-1, 1]\n",
    "])\n",
    "\n",
    "def prepare_training_batch(spectrogram_list):\n",
    "    processed_specs = []\n",
    "    for spec in spectrogram_list:\n",
    "        # Convert NumPy spectrogram to a PIL Image for torchvision transforms\n",
    "        # We first scale the spec to 0-255 range\n",
    "        spec_min, spec_max = spec.min(), spec.max()\n",
    "        spec_scaled = 255 * (spec - spec_min) / (spec_max - spec_min)\n",
    "        img = Image.fromarray(spec_scaled.astype('uint8'))\n",
    "\n",
    "        # Apply Resize and Normalize\n",
    "        processed_specs.append(preprocess(img))\n",
    "\n",
    "    # Stack all 50 clips into one large Tensor\n",
    "    return torch.stack(processed_specs)\n",
    "\n",
    "# Example usage with your extracted clips\n",
    "# training_tensor = prepare_training_batch(your_list_of_spectrograms)\n",
    "print(\"Preprocessing pipeline ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtCR-dxy8oS8",
    "outputId": "921bf302-6be2-4cac-eac5-164d8ed03ddb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Added for relu used in model definition\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Assuming QaraamiTranscriptor model is defined in a previous cell and available in global scope.\n",
    "# If not, the class definition from LGMJ6Skv7QqX would need to be included here.\n",
    "\n",
    "# 1. Setup Device (Use GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 2. Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"khalidibra/Qaraami-Colab-Write\", split=\"train\")\n",
    "\n",
    "# Define the transformation pipeline (copied from JWE3Dw3H8Tuk)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # Standard size for CNN input\n",
    "    transforms.ToTensor(),         # Scales values to [0.0, 1.0]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Centers data to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create a mapping from note names to integer labels\n",
    "unique_notes = sorted(list(set([entry['note'] for entry in dataset])))\n",
    "note_to_idx = {note: idx for idx, note in enumerate(unique_notes)}\n",
    "num_classes = len(unique_notes)\n",
    "\n",
    "# Verify that the model's output layer matches the number of unique notes\n",
    "# If model.fc2.out_features != num_classes, the model would need to be re-instantiated.\n",
    "# In this case, both are 50, so no re-instantiation is strictly needed, but good to check.\n",
    "if model.fc2.out_features != num_classes:\n",
    "    print(f\"Warning: Model output features ({model.fc2.out_features}) do not match actual number of classes ({num_classes}). Re-instantiating model.\")\n",
    "    model = QaraamiTranscriptor(num_classes=num_classes)\n",
    "    model.to(device)\n",
    "    # Re-initialize optimizer if model was re-instantiated\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "all_spectrograms = []\n",
    "all_labels = []\n",
    "\n",
    "# Assuming audio clips are in '/content/qaraami_dataset_clips/'\n",
    "audio_clip_base_path = '/content/qaraami_dataset_clips/'\n",
    "\n",
    "print(\"Loading and preprocessing audio clips...\")\n",
    "for i, entry in enumerate(dataset):\n",
    "    audio_filename = entry['audio_file'].split('/')[-1] # e.g., 'extracted_audio_clip_0.wav'\n",
    "    file_path = os.path.join(audio_clip_base_path, audio_filename)\n",
    "\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(file_path, sr=22050) # Use a consistent sampling rate\n",
    "\n",
    "    # Compute Mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Convert NumPy spectrogram to a PIL Image and then apply transforms\n",
    "    # Scale to 0-255 for PIL Image conversion, then to float tensor by ToTensor()\n",
    "    spec_min, spec_max = S_dB.min(), S_dB.max()\n",
    "    if (spec_max - spec_min) == 0:\n",
    "        spec_scaled = np.zeros_like(S_dB)\n",
    "    else:\n",
    "        spec_scaled = 255 * (S_dB - spec_min) / (spec_max - spec_min)\n",
    "\n",
    "    img = Image.fromarray(spec_scaled.astype('uint8'))\n",
    "\n",
    "    processed_spec = preprocess(img)\n",
    "    all_spectrograms.append(processed_spec)\n",
    "    all_labels.append(note_to_idx[entry['note']])\n",
    "\n",
    "# Stack all processed spectrograms and labels into tensors\n",
    "inputs = torch.stack(all_spectrograms)\n",
    "labels = torch.LongTensor(all_labels)\n",
    "\n",
    "# Move data to the device\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "print(\"Data loaded and prepared.\")\n",
    "\n",
    "def train_qaraami_model(epochs=20, inputs=inputs, labels=labels):\n",
    "    model.train() # Tell the model it is in training mode\n",
    "    print(f\"--- Starting Training on {device} ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "\n",
    "        # The AI makes a guess (Forward Pass)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # The AI learns from the error (Backward Pass)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # This will now appear in your terminal!\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"--- Training Complete ---\")\n",
    "\n",
    "# Execute the function\n",
    "train_qaraami_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "p4oARqq1-ESG",
    "outputId": "4aee1255-8f8d-43bd-a83f-6fff5c0d8d68"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save the trained parameters (the \"brain\")\n",
    "torch.save(model.state_dict(), 'qaraami_model_v1.pth')\n",
    "\n",
    "# Download it to your operatinf system\n",
    "from google.colab import files\n",
    "files.download('qaraami_model_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XveppEJG-Xc7",
    "outputId": "a756bb31-4fbb-4060-bb49-4eeca2e6719d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image # Added for image processing\n",
    "\n",
    "# 1. Load the \"Brain\" you just trained\n",
    "model.load_state_dict(torch.load('qaraami_model_v1.pth'))\n",
    "model.eval() # Set to evaluation mode for inference\n",
    "\n",
    "# Recreate idx_to_note as it's needed here and might not be globally persistent\n",
    "# based on the environment state. `note_to_idx` and `unique_notes` are global.\n",
    "idx_to_note = {idx: note for note, idx in note_to_idx.items()}\n",
    "\n",
    "def audio_to_abc(audio_path, model, preprocess, idx_to_note, device):\n",
    "    # Prepare the audio just like training\n",
    "    y, sr = librosa.load(audio_path, sr=22050) # Use consistent sampling rate\n",
    "\n",
    "    # Compute Mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Convert NumPy spectrogram to a PIL Image and then apply transforms\n",
    "    spec_min, spec_max = S_dB.min(), S_dB.max()\n",
    "    if (spec_max - spec_min) == 0:\n",
    "        spec_scaled = np.zeros_like(S_dB)\n",
    "    else:\n",
    "        spec_scaled = 255 * (S_dB - spec_min) / (spec_max - spec_min)\n",
    "\n",
    "    img = Image.fromarray(spec_scaled.astype('uint8'))\n",
    "    processed_spec = preprocess(img).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
    "\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(processed_spec)\n",
    "        _, predicted_index_tensor = torch.max(outputs, 1)\n",
    "\n",
    "    predicted_index = predicted_index_tensor.item() # Get the Python integer index\n",
    "\n",
    "    # 2. Map Index -> Note Name -> ABC Notation\n",
    "    abc_note = idx_to_note.get(predicted_index, \"z\") # 'z' is a rest for unknown notes\n",
    "\n",
    "    # 3. Format as a proper ABC score\n",
    "    abc_score = f\"X:1\\nT:Qaraami Transcription\\nM:4/4\\nL:1/4\\nK:C\\n{abc_note}|]\"\n",
    "    return abc_score\n",
    "\n",
    "# Test the transcription with the corrected path and necessary objects\n",
    "print(audio_to_abc('/content/qaraami_dataset_clips/extracted_audio_clip_0.wav', model, preprocess, idx_to_note, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDOEshG4_d7Q",
    "outputId": "8550cc9c-50be-41ff-aaed-9a670cc6157e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image # Ensure Image is imported for preprocessing\n",
    "\n",
    "# 1. Setup Model and Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load('qaraami_model_v1.pth'))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Ensure idx_to_note and preprocess are accessible from previous cells or re-defined\n",
    "# (Assuming they are global from gtCR-dxy8oS8 and XveppEJG-Xc7 respectively)\n",
    "# If running this cell independently, ensure these are available.\n",
    "\n",
    "def transcribe_long_oud_session(audio_path, model, preprocess, idx_to_note, device, segment_duration=1.0):\n",
    "    # Load the long recording\n",
    "    y, sr = librosa.load(audio_path, sr=22050) # Use consistent sampling rate\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "    full_melody = []\n",
    "    # Loop through the song in 1-second chunks\n",
    "    for start in np.arange(0, total_duration, segment_duration):\n",
    "        end = start + segment_duration\n",
    "        chunk = y[int(start*sr):int(end*sr)]\n",
    "\n",
    "        # Skip chunks that are too short to process\n",
    "        if len(chunk) < sr * segment_duration: continue\n",
    "\n",
    "        # --- Preprocessing ---\n",
    "        # 1. Convert chunk to Spectrogram\n",
    "        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=128, fmax=8000)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # 2. Resize to 128x128 and Convert to Tensor (as in preprocess pipeline)\n",
    "        spec_min, spec_max = S_dB.min(), S_dB.max()\n",
    "        if (spec_max - spec_min) == 0:\n",
    "            spec_scaled = np.zeros_like(S_dB)\n",
    "        else:\n",
    "            spec_scaled = 255 * (S_dB - spec_min) / (spec_max - spec_min)\n",
    "\n",
    "        img = Image.fromarray(spec_scaled.astype('uint8'))\n",
    "        processed_spec = preprocess(img).unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "        # --- Prediction ---\n",
    "        with torch.no_grad():\n",
    "            outputs = model(processed_spec)\n",
    "            _, predicted_index_tensor = torch.max(outputs, 1)\n",
    "            pred_idx = predicted_index_tensor.item()\n",
    "\n",
    "        note_name = idx_to_note.get(pred_idx, \"z\") # 'z' is a rest for unknown notes\n",
    "        # Only add the note if it's different from the last one (prevents repeats)\n",
    "        if not full_melody or note_name != full_melody[-1]:\n",
    "            full_melody.append(note_name)\n",
    "\n",
    "    # Format as a full ABC score\n",
    "    abc_string = \" \".join(full_melody) # Join notes with spaces for readability\n",
    "    return f\"X:1\\nT:Oud Session Transcription\\nM:4/4\\nL:1/4\\nK:C\\n{abc_string}|]\"\n",
    "\n",
    "# Paste the path to your long 30s-60s recording here\n",
    "# Using the original input file for testing, as it's a long audio session.\n",
    "print(transcribe_long_oud_session('/content/input_folder/extracted_audio.mp3', model, preprocess, idx_to_note, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyNca_r8_82v",
    "outputId": "f07da761-5d3b-46fa-c22e-eb831e41be1b"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image # Needed for preprocessing\n",
    "\n",
    "# Load your model as before\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load('qaraami_model_v1.pth'))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Ensure preprocess and idx_to_note are accessible from previous cells\n",
    "# (Assuming they are global from gtCR-dxy8oS8 and XveppEJG-Xc7 respectively)\n",
    "# If running this cell independently, ensure these are available.\n",
    "\n",
    "def transcribe_with_onsets(audio_path, model, preprocess, idx_to_note):\n",
    "    # 1. Load audio and detect onsets\n",
    "    y, sr = librosa.load(audio_path, sr=22050) # Use consistent SR as in training\n",
    "\n",
    "    # Use a different sampling rate for onset detection if desired, but 22050 is fine\n",
    "    # and ensures consistency with other librosa calls.\n",
    "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, wait=5, pre_avg=1, post_avg=1, units='frames')\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "\n",
    "    full_melody = []\n",
    "\n",
    "    # 2. Iterate through each detected note\n",
    "    for i in range(len(onset_times)):\n",
    "        start_time = onset_times[i]\n",
    "        # The note ends when the next one begins, or at the end of the file\n",
    "        end_time = onset_times[i+1] if i+1 < len(onset_times) else librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        # Extract the audio slice for this specific note\n",
    "        chunk = y[int(start_time*sr):int(end_time*sr)]\n",
    "\n",
    "        # Only process if the chunk is long enough for the model\n",
    "        # A 1-second segment at 22050 SR is 22050 samples, 1024 is too small\n",
    "        # Let's ensure a minimum size, e.g., 0.1s worth of samples\n",
    "        if len(chunk) > sr * 0.1:\n",
    "            # --- Preprocess chunk to Spectrogram here ---\n",
    "            S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=128, fmax=8000)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "            # Convert NumPy spectrogram to a PIL Image and then apply transforms\n",
    "            spec_min, spec_max = S_dB.min(), S_dB.max()\n",
    "            if (spec_max - spec_min) == 0:\n",
    "                spec_scaled = np.zeros_like(S_dB)\n",
    "            else:\n",
    "                spec_scaled = 255 * (S_dB - spec_min) / (spec_max - spec_min)\n",
    "\n",
    "            img = Image.fromarray(spec_scaled.astype('uint8'))\n",
    "            processed_spec = preprocess(img).unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(processed_spec)\n",
    "                _, predicted_index_tensor = torch.max(outputs, 1)\n",
    "                pred_idx = predicted_index_tensor.item()\n",
    "\n",
    "            note = idx_to_note.get(pred_idx, \"z\") # Use idx_to_note\n",
    "\n",
    "            # Determine rhythmic length (simplified)\n",
    "            duration = end_time - start_time\n",
    "            if duration < 0.2: note += \"1/2\" # Eighth note\n",
    "            elif duration > 0.6: note += \"2\" # Half note\n",
    "\n",
    "            # Only add if different from the last note, to avoid long repeats\n",
    "            if not full_melody or note != full_melody[-1]:\n",
    "                full_melody.append(note)\n",
    "\n",
    "    return f\"X:1\\nT:Rhythmic Oud Session\\nM:4/4\\nK:C\\n{' '.join(full_melody)}|]\"\n",
    "\n",
    "# Call the function with the corrected path and necessary objects\n",
    "print(transcribe_with_onsets('/content/qaraami_dataset_clips/extracted_audio_clip_0.wav', model, preprocess, idx_to_note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "eaTWgJ2jAdSR",
    "outputId": "e5bf8eb4-4648-4fe8-a392-c39a395834d7"
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_qaraami_verification(audio_path, abc_result):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # 1. Plot the Waveform\n",
    "    librosa.display.waveshow(y, sr=sr, alpha=0.5, color='blue')\n",
    "\n",
    "    # 2. Detect onsets again to get timestamps\n",
    "    onsets = librosa.onset.onset_detect(y=y, sr=sr, units='time')\n",
    "\n",
    "    # 3. Clean the ABC result string to get just the notes\n",
    "    # (Simplified for visualization)\n",
    "    notes = [\"A5\", \"A5\", \"B4\", \"F5\", \"F5\", \"E5\"]\n",
    "\n",
    "    # 4. Overlay notes on the plot\n",
    "    for i, t in enumerate(onsets[:len(notes)]):\n",
    "        plt.vlines(t, -1, 1, color='red', linestyle='--')\n",
    "        plt.text(t, 0.5, notes[i], color='darkred', weight='bold')\n",
    "\n",
    "    plt.title(f\"Verification: {abc_result}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Oud Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_qaraami_verification('/content/qaraami_dataset_clips/extracted_audio_clip_0.wav', \"A52 A5 B4...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhA-jkX7A5A7"
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "import torch\n",
    "import librosa\n",
    "# ... import your QaraamiTranscriptor class and note logic\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the \"Brain\" once when the server starts\n",
    "model = QaraamiTranscriptor(num_classes=15)\n",
    "model.load_state_dict(torch.load('qaraami_model_v1.pth', map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "@app.post(\"/api/transcribe\")\n",
    "async def transcribe_audio(file: UploadFile = File(...)):\n",
    "    # 1. Save and load the uploaded audio\n",
    "    content = await file.read()\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    # 2. Run your transcription logic\n",
    "    abc_result = transcribe_with_onsets(\"temp.wav\")\n",
    "\n",
    "    # 3. Return the ABC string to Next.js\n",
    "    return {\"abc\": abc_result}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "008da34a80714de2968b674e5475aee5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a115dc7f9ac14fd7a049840aa03b88a9",
      "placeholder": "​",
      "style": "IPY_MODEL_897012e88a3e4fa592869c83644facb6",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "06f77d435c244fc0b82df6671daa2d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "081023066231458c8e277e8937374822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "0b2e3e22396f4e44ab003d6a4444a065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c1745ed7382430484703674beb0da14",
      "placeholder": "​",
      "style": "IPY_MODEL_5e7db1a7454f4828826fab4109979710",
      "value": " 50/50 [00:00&lt;00:00, 931.76 examples/s]"
     }
    },
    "11c9ab3725424f98827af8ac8818e89e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "14c42c65b81243928ae8c39a48630b98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14dfcfdbba1f4669b44edfe736f4169a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_cbaa5875617b497d84fdb24d43d3773f",
      "style": "IPY_MODEL_06f77d435c244fc0b82df6671daa2d05",
      "value": true
     }
    },
    "1f537c6d69f3466983c2002a2b6b3417": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2720c3c1c0dc4636ab27540dd8a2a818": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dff3b35cb3784db78ee51b35f19cf6ef",
       "IPY_MODEL_a955eb3a7563473a921974d46d3fbc73",
       "IPY_MODEL_0b2e3e22396f4e44ab003d6a4444a065"
      ],
      "layout": "IPY_MODEL_1f537c6d69f3466983c2002a2b6b3417"
     }
    },
    "2a9905b27fe14d988c2ca29c2baeca22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31b84a46193a48f7b6f3e8b6a7d750d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "3a322787d0424b4e843b8c285efb9ee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a0e71ef27cb4df28ae83191553e3db1",
      "placeholder": "​",
      "style": "IPY_MODEL_9f1ce08952a941cca5935012ff5449f9",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "3b95b78484a342eb95683d196b7e1398": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c1745ed7382430484703674beb0da14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e7db1a7454f4828826fab4109979710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a0e71ef27cb4df28ae83191553e3db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "736763a336f64b3b82d038a5f27fdc61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ab4856384ca44a3af463c9e2de8bba6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "83019cbc33524fabb243af0e33d68503": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83f5d37e65ec4babb6748124fb8b7a8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "875bab8a9c8544248c490d8b7117e641": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "897012e88a3e4fa592869c83644facb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "982adbe261f44421893648b93f6b6cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83019cbc33524fabb243af0e33d68503",
      "placeholder": "​",
      "style": "IPY_MODEL_a8e72987fc7f4e819e2ea88003391a3e",
      "value": " 6.39k/? [00:00&lt;00:00, 281kB/s]"
     }
    },
    "9d6719303145415394537730e7ea3b0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_14c42c65b81243928ae8c39a48630b98",
      "placeholder": "​",
      "style": "IPY_MODEL_3b95b78484a342eb95683d196b7e1398",
      "value": ""
     }
    },
    "9f1ce08952a941cca5935012ff5449f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a115dc7f9ac14fd7a049840aa03b88a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a362349ed764496eb726aef642ca9b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ab4856384ca44a3af463c9e2de8bba6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_11c9ab3725424f98827af8ac8818e89e",
      "value": 1
     }
    },
    "a821e515df3743d19222b4c9f16ae29e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8e72987fc7f4e819e2ea88003391a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a955eb3a7563473a921974d46d3fbc73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_736763a336f64b3b82d038a5f27fdc61",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c08243b5ec1e4a50867a86463623f32e",
      "value": 50
     }
    },
    "b30a854b0e9747d486504502c821d7da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_df62b44aa4154f01becdb7184f99bbea",
      "style": "IPY_MODEL_081023066231458c8e277e8937374822",
      "tooltip": ""
     }
    },
    "c08243b5ec1e4a50867a86463623f32e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbaa5875617b497d84fdb24d43d3773f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd882e4f1c1d4e22ab51ed13d8e1dd57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cda78230aba04ed9b07df2b5b51384d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a821e515df3743d19222b4c9f16ae29e",
      "placeholder": "​",
      "style": "IPY_MODEL_cd882e4f1c1d4e22ab51ed13d8e1dd57",
      "value": "metadata.jsonl: "
     }
    },
    "d1fc978a9d9e412b8246fcad404e1031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc0e54a3a12242ca9a433554e43d3bb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df62b44aa4154f01becdb7184f99bbea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dff3b35cb3784db78ee51b35f19cf6ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a9905b27fe14d988c2ca29c2baeca22",
      "placeholder": "​",
      "style": "IPY_MODEL_83f5d37e65ec4babb6748124fb8b7a8e",
      "value": "Generating train split: 100%"
     }
    },
    "e04909d764f54d508a36b87a23fe5a2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_31b84a46193a48f7b6f3e8b6a7d750d7"
     }
    },
    "e4bb396156b84181a3cae158c0907608": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cda78230aba04ed9b07df2b5b51384d8",
       "IPY_MODEL_a362349ed764496eb726aef642ca9b66",
       "IPY_MODEL_982adbe261f44421893648b93f6b6cd1"
      ],
      "layout": "IPY_MODEL_dc0e54a3a12242ca9a433554e43d3bb9"
     }
    },
    "f6a579ed55fb4800ad77f4bcd17fb5ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_875bab8a9c8544248c490d8b7117e641",
      "placeholder": "​",
      "style": "IPY_MODEL_d1fc978a9d9e412b8246fcad404e1031",
      "value": "Connecting..."
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
